# 00 실패 원인으로 우회에 대한 패널티가 적은 것 같았다. 전체적으로 보상 scale을 축소하고, gamma를 0.9로 줄였다. truncation limit을 100으로 줄여서 혼란을 줄 수 있을법한 움직임들을 줄여보고자 했다.
# === arguments: Run ===
env_id: "gym_maze/Maze-v0"
exp_name: "dqn_01"
seed: 42
torch_deterministic: True
cuda: False
num_envs: 1             # '1'만 가능

# === arguments: Env ===
height_range: [3, 4]
width_range: [3, 5]
reward:
  goal: [1, null]
  friction: [null, 0.05]    #0.1]
  #manhattan_dist: [0.15, null]
  shortest_path: [0.1, null]

learning_rate: 0.0001
buffer_size: 20000        #100000
total_timesteps: 1000000  #10000000
learning_starts: 20000    #80000
train_frequency: 4
truncation_limit: 100     #1000

save_frequency: 100000
eval_frequency: 10000

# arguments: Epsilon Greedy
start_e: 1
end_e: 0.01
exploration_fraction: 0.10

# arguments: Training
batch_size: 32
gamma: 0.9  #0.99
target_network_frequency: 1000
tau: 1.0

# arguments: Evaluation
n_episode_eval: 20